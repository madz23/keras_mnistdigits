# -*- coding: utf-8 -*-
"""Homework_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sr2WyVw6CklBMDAnHLxyzqk35q4KUXKn

# Homework 3: Cats and Dogs
Applied Neural Networks <br/>
Dr. Leslie Kerby

Kaggle contains many useful datasets and data science competitions. It also has great tutorials and discussion boards. The data for this assignment comes from the [Kaggle Cats vs Dogs competition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/overview). Images such as these were once used for CAPTCHA (after digits and the alphabet proved too crackable). As stated in the overview, many years ago computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art (you should do better than this even without using CNNs). However, even back in 2014 state of the art machine learning could exceed 80% accuracy on this cat and dog dataset. This meant it was no longer useful for CAPTCHA. Currently, with the utilization of transfer learning this accuracy can exceed 95%.

**Part 1** <br/>
Download the dataset. Go to [this link](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/) and set up your computer and Colab to easily download Kaggle datasets. Then download the `dogs-vs-cats-redux-kernels-edition` dataset. <br/>
*Note: You may set up the dataset another way (run jupyter locally etc) if you prefer*
"""


# imports
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf

import keras_tuner as kt
from tensorflow import keras

#google keras tuner for classification 

# import kaggle
from google.colab import files
files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# make sure Kaggle is imported correctly
! kaggle datasets list

! kaggle competitions download -c dogs-vs-cats-redux-kernels-edition

# made a directory for our dataset
! mkdir train

! unzip train.zip -d train

"""**Part 2**<br/>
Preprocess the dataset (load it into one dataframe and create your `y` labels). <br/> 
Print 5 sample images of dogs and cats (BEFORE they have been resized).
"""

# You may find the following code hints useful

import os
import pandas as pd


filenames = os.listdir("train/train/")
categories = []
for filename in filenames:
    category = filename.split('.')[0]
    if category == 'cat':
        categories.append(1)
    else:
        categories.append(0)

df = pd.DataFrame({
    'filename': filenames,
    'category': categories
})
# Note: this randomizes the filenames

df.head(5)

import matplotlib.pyplot as plt

import cv2
img = cv2.imread('train/train/'+df['filename'][1349], cv2.IMREAD_COLOR)
plt.imshow(img)

img = cv2.imread('train/train/'+df['filename'][0], cv2.IMREAD_COLOR)
plt.imshow(img)

"""You can then take `df` and create `y` from it and add each image to `X` (in order as found in df). Note that the images are all different sizes -- so we will need to resize and interpolate them before inputing them in Keras."""

# We haven't covered this yet so I'm giving you the code 
import cv2
import numpy as np

def resize_img(file_path):
  img = cv2.imread(file_path, cv2.IMREAD_COLOR)
  return cv2.resize(img, (28,28))  # This tuple represents the final image size desired

X = np.ndarray((25000,28,28,3)) # 25000 images, 80x80 pixels, 3 channels (RGB) 
for i,image_file in enumerate(df['filename']):
    X[i,:] = resize_img('train/train/'+image_file)

"""**Part 3**<br/>
Now split the training data into training (15000 images), validation (5000 images), and testing (5000 images) datasets.
"""

X_train_full = X
y_train_full = df['category'].to_numpy(dtype='int8')

# Create validation dataset
from sklearn.model_selection import train_test_split


x_train_t, x_train_v, y_train_t, y_train_v = train_test_split(X_train_full, y_train_full, test_size = 5000)

"""**Part 4**<br/>
Try different NN architectures and options. Use KerasTuner (or alternatively the sklearn tools RandomizedSearchCV or GridSearchCV) as part of this. Try at least one deep neural network with at least 50 hidden layers. Clearly state initialization, activation, architecture (including # layers and neurons, and pathways), any normalization/regularization used, and other relevant information for each model. 

You are NOT expected to utilize transfer learning, data augmentation, or convolutional neural networks (these will be added in a future assignment). 

Give converged validation and testing accuracy for each model trained (utilizing the EarlyStopping callback). Which one was the best? Display five misclassified images from your best model. Why do you think it was the best? Note: Your grade for this homework will depend on the quality of your best model.
"""

# Model 1: Keras tuner


# Here I will define a model class that can be applied later. This was taken from the Tensorflow website
# https://www.tensorflow.org/tutorials/keras/keras_tuner#define_the_model

import keras_tuner as kt
from tensorflow import keras
import tensorflow as tf

def model_builder(hp):
  model = keras.Sequential()
  model.add(keras.layers.Dense(
      hp.Int('units', min_value=8, max_value=32, step=8),
      activation='relu'))
  model.add(keras.layers.Dense(
      hp.Choice('units_2', [8, 16, 32]), 
      activation='relu'))
  model.add(keras.layers.Dense(1))

  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

  return model

# Now we can make the tuner. We will use Hyperband this time
tuner = kt.Hyperband(model_builder,
                     objective='val_loss',
                     max_epochs=10,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

tuner.search(x_train_t, y_train_t, epochs=20, validation_split=0.2, callbacks=[keras.callbacks.EarlyStopping(patience=5)])
# Note: this is not building or training the model. Here, keras tuner finds the best hyper parameter values for us to use

tuner.results_summary() # This looks good, we should be able to build our model now

# Get the best model from the search
best_hps = tuner.get_best_hyperparameters()[0]
model = tuner.hypermodel.build(best_hps)

# Now we can train our model
model.fit(x_train_t, y_train_t, epochs=50, validation_data=(x_train_v, y_train_v),
             callbacks=[keras.callbacks.EarlyStopping(patience=5)])

history.history

eval_result = model.evaluate(x_train_t, y_train_t)
print("[test loss, test accuracy]:", eval_result)

"""**Graduate students only**<br/>
Prepare a submission to Kaggle by making predictions on the test data. Submit a screenshot of your submission and accuracy results.
"""



